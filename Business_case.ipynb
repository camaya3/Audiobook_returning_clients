{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92cf173-6380-412e-a908-8d38f68a4ac4",
   "metadata": {},
   "source": [
    "## Business Case Tensor Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0428af7-66f7-4ffd-891f-5d2fe657d33b",
   "metadata": {},
   "source": [
    "#### Practical example. Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ec760-bf01-4431-bd98-7d3e56f19efc",
   "metadata": {},
   "source": [
    "#### Prepocess the data. Balance the dataset. Create 3 datasets; training, validation and testing. Save the newly created sets in a tensor format (e.g *.npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef591acd-5b58-4150-8ebb-609b2ed4d29e",
   "metadata": {},
   "source": [
    "Since we are dealing with real life data, we will need to preprocess it a bit. This is the relecant code, which is not that hard, but refers to data engineering more than machine learning. \n",
    "\n",
    "If you want to know how to do that, go through the code and the comments. In any case, this should do the trick for all datasets organized in the way: many inputs, and then 1 cell containing the targets (all supervised learning datasets). \n",
    "\n",
    "Note that we have removed the header now, which containes the names of the categories. We simply want the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730dd9ca-8301-421c-bfa7-eda09ab4a495",
   "metadata": {},
   "source": [
    "### Extract the data from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18842bc-29c0-4c32-9d79-1108bdaecfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing #we use sk learn the standardize the inputs. Almost always we standardize the inputs\n",
    "                                  # Standardizing gains 10% accuracy in this problem\n",
    "raw_csv_data = np.loadtxt(\"C:/Users/camay/OneDrive/Desktop/Data Science Bootcamp/Audiobooks_data.csv\", delimiter = ',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31b43e-e8f6-49d5-8fff-73fdd1a9488c",
   "metadata": {},
   "source": [
    "### Balance the datasheet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbef98f-dcd5-436a-aacb-244cfc36754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of 1's\n",
    "num_ones_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove=[]\n",
    "\n",
    "for i in range(targets_all.shape[0]):  # The shape of targets_all on axis=0, is basically the lengthg of the vector\n",
    "    if targets_all[i] == 0:    # We want to increase the zeroes counter by 1, if the target is 0\n",
    "        zero_targets_counter +=1\n",
    "        if zero_targets_counter > num_ones_targets:   #If the target at position i is 0, and the number of zeroes is bigger than the number of 1's\n",
    "            indices_to_remove.append(i)                  # we will know the indices of all data points to be removed\n",
    "\n",
    "unscaled_input_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)  #deletes an object along an axis\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0) #Same thing with the targets column (vector) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234e0b1-779b-4f64-a267-426bf7925a4f",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3cd940-0f6d-4d1f-a4f9-3b3130329e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_input_equal_priors) #standardizes an array along an axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28faebb6-12e5-48c2-b6b2-8d64d7ed3315",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be075c9a-abf1-4f8a-ac5e-237f3b53b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are batching, we must shuffle the data\n",
    "\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0]) #method that returns a evenly spaced calues within a given interval\n",
    "np.random.shuffle(shuffled_indices)  #method that shuffles the numbers in a given sequence\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a0d79-076e-4521-837f-b5e175635077",
   "metadata": {},
   "source": [
    "### Split the dataset into train=, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869ef48a-1275-4e90-92ba-eb89c8c01b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751.0 3579 0.48924280525286395\n",
      "242.0 447 0.5413870246085011\n",
      "244.0 448 0.5446428571428571\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Lets split the data 80% train, 10% validation and 10% test\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_count = int(.1 * samples_count)\n",
    "test_count = samples_count - train_samples_count - validation_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count + validation_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count + validation_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_count:]\n",
    "\n",
    "\n",
    "# It is useful to check if we have balanced the dataset\n",
    "\n",
    "print(np.sum(train_targets),train_samples_count, np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets), validation_count, np.sum(validation_targets)/validation_count)\n",
    "print(np.sum(test_targets),test_count, np.sum(test_targets)/test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e20f8-0d1c-49f8-a5ca-26c2ac4aa53d",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71f9888-2be1-496f-b1dd-72a54fc9ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets= validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a653ed80-36db-487b-ac70-afe24d7f9184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\camay\\\\Data Science Bootcamp 2024\\\\Section 53'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516d700-ca62-45a9-836b-bab3f5a98e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.TF2.0]",
   "language": "python",
   "name": "conda-env-py3.TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
